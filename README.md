# **B2C Transactional Data Pipeline with AWS, Terraform, and Airflow**

## **ðŸ“Œ Project Overview**
This project demonstrates an **end-to-end ETL pipeline** for **Business-to-Customer (B2C) transaction data** using:
- **Terraform** for Infrastructure as Code (IaC) on AWS  
- **Amazon S3** as staging storage  
- **Amazon Redshift** as the data warehouse  
- **Apache Airflow** for orchestration  
- **Faker** for synthetic data generation  

The pipeline:
1. **Generates synthetic B2C transaction data** with Faker in Python.  
2. **Saves the data to S3** in **Parquet** format using AWS Wrangler and `boto3` for the session..  
3. **Loads data from S3 to Amazon Redshift** using Airflow's `S3ToRedshiftOperator`.  
4. **Automates infrastructure provisioning** (VPC, subnets, IAM roles, S3 buckets, Redshift cluster) via Terraform.  

---

## **ðŸ›  Tech Stack**
- **Infrastructure:** Terraform, AWS (VPC, S3, IAM, Redshift)  
- **Data Orchestration:** Apache Airflow (CeleryExecutor, Redis, PostgreSQL backend)  
- **Data Generation:** Python, Faker, Pandas, AWS Wrangler  
- **Containerization:** Docker, Docker Compose  
- **Programming Languages:** Python, HCL (Terraform)  

---

## **ðŸ“‚ Project Structure**
```plaintext
.
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ b2c_dags.py               # Airflow DAG definition
â”‚   â””â”€â”€ b2c_trans_data.py         # Python script to generate synthetic data
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ main.tf                   # AWS resources definition
|    â””â”€â”€ redshift_vpc_stack.tf 
â”‚   â””â”€â”€ variables.tf              # Terraform variables
|    â””â”€â”€ provider.tf
|    â””â”€â”€ backend.tf
|    â””â”€â”€ s3_bucket.tf
â”œâ”€â”€ Dockerfile                    # Custom Airflow image
â”œâ”€â”€ docker-compose.yaml           # Airflow multi-service deployment
â”œâ”€â”€ requirements.txt              # Python dependencies





â””â”€â”€ README.md

